{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sana-Harshitha/LLMPractice/blob/main/FineTuning_LLM_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwhsqvelWD3s",
        "outputId": "e582169a-e4ec-4016-919e-95006c30cb56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import ssl\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Create an unverified SSL context\n",
        "    ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "g9__aLn5WUOI",
        "outputId": "0aac84cf-f8cf-4a95-e4f3-1df1b3b7a67c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Label                                               Text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6GRiaxfWykc",
        "outputId": "2932e79f-110c-4489-9a98-e032d98cd4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlkDGHvOW5NF",
        "outputId": "9c1db534-f8c5-4471-e456-509b445eb136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df):\n",
        "\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vTa1XVpXW9eD"
      },
      "outputs": [],
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c8e9yDxZXMT6"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "# Test size is implied to be 0.2 as the remainder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqKEb5cWXSWr",
        "outputId": "884bb337-e974-43b8-ce20-ec68c9ba3463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1045\n",
            "149\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df))\n",
        "print(len(validation_df))\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u5NCWzgagO8C"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gCyzjhTYXVrj"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "# Initialize the tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccRBGoEoVtqu"
      },
      "source": [
        "### CREATING DATALOADERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15G7gXoyVtqu"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Previously, we utilized a sliding window technique to generate uniformly\n",
        "sized text chunks, which were then grouped into batches for more efficient model training.\n",
        "Each chunk functioned as an individual training instance\n",
        "\n",
        "In the case of email spam classification, have two primary options:\n",
        "\n",
        "(1) Truncate all messages to the length of the shortest message in the\n",
        "dataset or batch.\n",
        "\n",
        "(2) Pad all messages to the length of the longest message in the dataset or\n",
        "batch.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF7q4YLjVtqu"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Option 1 is computationally cheaper, but it may result in significant information loss if\n",
        "shorter messages are much smaller than the average or longest messages, potentially\n",
        "reducing model performance.\n",
        "\n",
        "So, we opt for the second option, which preserves the entire\n",
        "content of all messages.\n",
        "\n",
        "To implement option 2, where all messages are padded to the length of the longest\n",
        "message in the dataset, we add padding tokens to all shorter messages.\n",
        "\n",
        "For this purpose,\n",
        "we use \"<|endoftext|>\" as a padding token, as discussed in chapter 2.\n",
        "\n",
        "    \n",
        "However, instead of appending the string \"<|endoftext|>\" to each of the text messages\n",
        "directly, we can add the token ID corresponding to \"<|endoftext|>\" to the encoded text\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umn6RdmXVtqu"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "As we have seen earlier, we first need to implement a PyTorch Dataset, which\n",
        "specifies how the data is loaded and processed, before we can instantiate the data loaders.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nry3oc1Vtqu"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "For this purpose, we define the SpamDataset class.\n",
        "\n",
        "This SpamDataset class handles several key tasks: it identifies the\n",
        "longest sequence in the training dataset, encodes the text messages, and ensures that all\n",
        "other sequences are padded with a padding token to match the length of the longest\n",
        "sequence.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5yn4dwQMVtqv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgXYFDifVtqv"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Step 1: Pre-tokenize texts\n",
        "    \n",
        "Step 2: Truncate sequences if they are longer than max_length\n",
        "    \n",
        "Step 3: Pad sequences to the longest sequence\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1UOLg6LVtqv"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "The SpamDataset class loads data from the CSV files we created earlier, tokenizes the text\n",
        "using the GPT-2 tokenizer from tiktoken and allows us to pad or truncate the sequences to\n",
        "a uniform length determined by either the longest sequence or a predefined maximum\n",
        "length.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puKZFcmFVtqv"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "This ensures each input tensor is of the same size, which is necessary to create the\n",
        "batches in the training data loader we implement next:\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3yVySDrVtqw",
        "outputId": "8469fcf3-cf3d-45a3-9acf-08bb581fab04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJVI8bjGVtqw"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "The code outputs 120, showing that the longest sequence contains no more than 120\n",
        "tokens, a common length for text messages.\n",
        "                       \n",
        "It's worth noting that the model can handle\n",
        "sequences of up to 1,024 tokens, given its context length limit.\n",
        "\n",
        "If your dataset includes\n",
        "longer texts, you can pass max_length=1024 when creating the training dataset in the\n",
        "preceding code to ensure that the data does not exceed the model's supported input\n",
        "(context) length.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUP39zATVtqw"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Next, we pad the validation and test sets to match the length of the longest training\n",
        "sequence.\n",
        "\n",
        "It's important to note that any validation and test set samples exceeding the\n",
        "length of the longest training example are truncated using\n",
        "encoded_text[:self.max_length] in the SpamDataset code we defined earlier.\n",
        "\n",
        "This\n",
        "truncation is optional; you could also set max_length=None for both validation and test\n",
        "sets, provided there are no sequences exceeding 1,024 tokens in these sets\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5FmeUoFpVtqx"
      },
      "outputs": [],
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQAaKe3QVtqx"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Using the datasets as inputs, we can instantiate the data loaders similarly to what we did earlier.\n",
        "\n",
        "However, in this case, the targets represent class labels rather than the next\n",
        "tokens in the text.\n",
        "\n",
        "For instance, choosing a batch size of 8, each batch will consist of 8\n",
        "training examples of length 120 and the corresponding class label of each example.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GeQ7bK4aVtqy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7o5M87pVtqy"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "To ensure that the data loaders are working and are indeed returning batches of the\n",
        "expected size, we iterate over the training loader and then print the tensor dimensions of\n",
        "the last batch:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwup5Yi5Vtqy",
        "outputId": "8f4e2ae4-7465-451e-f3d8-35ebc98eb3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma6FTHB7Vtqz"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see, the input batches consist of 8 training examples with 120 tokens each, as\n",
        "expected.\n",
        "\n",
        "The label tensor stores the class labels corresponding to the 8 training examples.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHgtP_kYVtq0"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Lastly, to get an idea of the dataset size, let's print the total number of batches in each\n",
        "dataset:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9OvYx5nVtq0",
        "outputId": "6f658343-a56a-4d41-c86b-6971f15c2c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wMprMntVtq0"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "This concludes the data preparation. Next, we will prepare the model for\n",
        "finetuning.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gcC8PAhVtq1",
        "outputId": "fdd241c8-9fc4-458d-965b-6da2131abca5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'drop_rate': 0.0,\n",
              " 'qkv_bias': True,\n",
              " 'emb_dim': 768,\n",
              " 'n_layers': 12,\n",
              " 'n_heads': 12}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "BASE_CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zd3-dhkTfcD8"
      },
      "outputs": [],
      "source": [
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eH_0SupFfn57"
      },
      "outputs": [],
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'124M'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'LLM_core'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLLM_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPTModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTModel\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'LLM_core'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "\n",
        "from LLM_core.GPTModel import GPTModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNBtZjNEF7zHjJJSQikmy0q",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
