{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLK7a4xm0kalYr7Wn/c68p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sana-Harshitha/LLMPractice/blob/main/trainable_weights_vs_non_trainable_weights_in_context_vector_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When generating word or sentence vectors, the core difference lies in whether the model uses trainable weights or not. In non-trainable methods ‚Äî like Word2Vec, GloVe, or fastText ‚Äî each word is assigned a fixed vector that doesn‚Äôt change based on the context. These embeddings are typically pre-trained on a large corpus and act like a dictionary: for example, the word \"cat\" in ‚ÄúThe cat sat on the mat‚Äù will have the exact same vector as \"cat\" in ‚ÄúThe cat chased a mouse‚Äù. The model doesn‚Äôt know anything about the sentence ‚Äî it only retrieves the static vector associated with \"cat\". These vectors are good for capturing general word meanings and semantic similarity, but they can‚Äôt distinguish between different uses of the same word.\n",
        "\n",
        "In contrast, vector generation with trainable weights ‚Äî as in models like BERT, GPT, or LSTM-based sentence encoders ‚Äî dynamically creates embeddings based on the sentence. These models are either trained or fine-tuned to adapt their internal weights so that the vector for a word reflects its meaning in the surrounding context. So, in ‚ÄúThe cat sat on the mat‚Äù, the word \"cat\" gets a vector that reflects its association with \"sat\" and \"mat\". But in ‚ÄúThe cat chased a mouse‚Äù, the vector for \"cat\" will change because it's now interacting with action words like \"chased\" and nouns like \"mouse\". This makes contextual embeddings powerful, as they allow models to understand nuanced meaning and ambiguity in language.\n",
        "\n",
        "In short: non-trainable embeddings treat \"cat\" the same everywhere, while trainable models understand that \"cat\" can behave differently depending on what it‚Äôs doing or who it‚Äôs with ‚Äî and generate different vectors accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "VLVBSY_JJ-W8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PNoAiTG9J7VF"
      },
      "outputs": [],
      "source": [
        "# üîÑ Fix any version issues by reinstalling required libraries\n",
        "!pip install -U numpy==1.23.5 scipy==1.10.1 gensim==4.3.2 transformers -q\n",
        "\n",
        "import gensim.downloader as api\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load GloVe (Static Embeddings)\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "vector_cat_static = glove_vectors['cat']\n",
        "\n",
        "# Show first 10 dimensions\n",
        "print(\"üîµ Static vector for 'cat':\")\n",
        "print(vector_cat_static[:10])\n",
        "\n",
        "# Load BERT (Contextual Embeddings)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "sentence1 = \"The cat sat on the mat.\"\n",
        "sentence2 = \"The cat chased the mouse.\"\n",
        "\n",
        "inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
        "inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs1 = model(**inputs1)\n",
        "    outputs2 = model(**inputs2)\n",
        "\n",
        "# BERT token index for 'cat' (may vary if tokenized differently)\n",
        "cat_vec1 = outputs1.last_hidden_state[0][2]\n",
        "cat_vec2 = outputs2.last_hidden_state[0][2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qd0cuPdSLJ-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}