{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCoD8uAogS/4gm2u14LDp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sana-Harshitha/LLMPractice/blob/main/TopK_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "7tQAT_oKtBl0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}"
      ],
      "metadata": {
        "id": "mVnahrLRtTG3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logits = torch.tensor(\n",
        "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")"
      ],
      "metadata": {
        "id": "CNxGJUnItVd4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6jDhRJJpbQ6"
      },
      "source": [
        "### DECODING STRATEGY 2: Top-k sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_mXwoH8pbQ6"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In the previous section, we implemented a probabilistic sampling approach coupled with\n",
        "temperature scaling to increase the diversity of the outputs.\n",
        "\n",
        "We saw that higher\n",
        "temperature values result in more uniformly distributed next-token probabilities, which\n",
        "result in more diverse outputs as it reduces the likelihood of the model repeatedly selecting\n",
        "the most probable token.\n",
        "\n",
        "This method allows for exploring less likely but potentially more\n",
        "interesting and creative paths in the generation process.\n",
        "\n",
        "However, One downside of this\n",
        "approach is that it sometimes leads to grammatically incorrect or completely nonsensical\n",
        "outputs such as \"every effort moves you pizza\".\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC3w5Wd_pbQ7"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In this section, we introduce another concept called top-k sampling, which, when\n",
        "combined with probabilistic sampling and temperature scaling, can improve the text\n",
        "generation results.\n",
        "\n",
        "In top-k sampling, we can restrict the sampled tokens to the top-k most likely tokens\n",
        "and exclude all other tokens from the selection process by masking their probability scores.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hhpKjK7cpbQ7",
        "outputId": "cf6b1f0c-bec6-4876-fb3f-7426bf1be783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
            "Top positions: tensor([3, 7, 0])\n"
          ]
        }
      ],
      "source": [
        "top_k = 3\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "print(\"Top logits:\", top_logits)\n",
        "print(\"Top positions:\", top_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9H5Qyt2pbQ8"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Subsequently, we apply PyTorch's where function to set the logit values of tokens that are\n",
        "below the lowest logit value within our top-3 selection to negative infinity (-inf).\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VyJyb6_GpbQ8",
        "outputId": "a8f94c6d-3522-461d-b23f-d12689fc6296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
          ]
        }
      ],
      "source": [
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    input=torch.tensor(float(\"-inf\")),\n",
        "    other=next_token_logits\n",
        ")\n",
        "\n",
        "print(new_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqvQwj_NpbQ8"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Lastly, let's apply the softmax function to turn these into next-token probabilities:\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d4Kq68kEpbQ9",
        "outputId": "8942ee5b-b289-4d0f-d99f-e621f7f40bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ],
      "source": [
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "print(topk_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4nraF-LpbQ9"
      },
      "source": [
        "### Merge Temperature Scaling and Top-k sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nanFSnlpbQ9"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "We can now apply the temperature scaling and multinomial function for probabilistic\n",
        "sampling introduced in the previous section to select the next token among these 3 nonzero probability scores to generate the next token. We do this in the next section by\n",
        "modifying the text generation function.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DepFgtTtpbQ-"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The previous two subsections introduced two concepts to increase the diversity of LLMgenerated text: temperature sampling and top-k sampling. In this section, we combine and\n",
        "add these concepts to modify the generate_simple function we used to generate text via\n",
        "the LLM earlier, creating a new generate function:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNXT1F7DpbQ-"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Step 1: For-loop is the same as before: Get logits, and only focus on last time step\n",
        "\n",
        "Step 2: In this new section, we filter logits with top_k sampling\n",
        "\n",
        "Step 3: This is the new section where we apply temperature scaling\n",
        "    \n",
        "Step 4: Carry out greedy next-token selection as before when temperature scaling is disabled\n",
        "\n",
        "Step 5: Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eXou7vJApbQ-"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    }
  ]
}