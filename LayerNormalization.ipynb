{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdj4vZ8hzgNbvYqwEPtDtX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sana-Harshitha/LLMPractice/blob/main/LayerNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fS4spYs7-JZJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-6\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm_x+self.shift"
      ],
      "metadata": {
        "id": "idgR6kv2-S1b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npd5BbouAmIH"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "This specific implementation of layer Normalization operates on the last dimension of the\n",
        "input tensor x, which represents the embedding dimension (emb_dim).\n",
        "\n",
        "The variable eps is a\n",
        "small constant (epsilon) added to the variance to prevent division by zero during\n",
        "normalization.\n",
        "\n",
        "The scale and shift are two trainable parameters (of the same dimension\n",
        "as the input) that the LLM automatically adjusts during training if it is determined that\n",
        "doing so would improve the model's performance on its training task.\n",
        "\n",
        "This allows the model\n",
        "to learn appropriate scaling and shifting that best suit the data it is processing.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCaNvTmgAmII"
      },
      "source": [
        "_A small note on biased variance_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyte421PAmII"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "In our variance calculation method, we have opted for an implementation detail by\n",
        "setting unbiased=False.\n",
        "\n",
        "For those curious about what this means, in the variance\n",
        "calculation, we divide by the number of inputs n in the variance formula.\n",
        "\n",
        "This approach does not apply Bessel's correction, which typically uses n-1 instead of n in\n",
        "the denominator to adjust for bias in sample variance estimation.\n",
        "\n",
        "This decision results in a so-called biased estimate of the variance.\n",
        "\n",
        "For large-scale language\n",
        "models (LLMs), where the embedding dimension n is significantly large, the\n",
        "difference between using n and n-1 is practically negligible.\n",
        "\n",
        "We chose this approach to ensure compatibility with the GPT-2 model's normalization layers and because it\n",
        "reflects TensorFlow's default behavior, which was used to implement the original GPT2 model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOD3KM3yAmII"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's now try the LayerNorm module in practice and apply it to the batch input:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.rand(5,10,3)\n",
        "emb_dim=x.shape[-1]\n",
        "layer_norm=LayerNormalization(emb_dim)"
      ],
      "metadata": {
        "id": "OIDtJXQM_ffy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=layer_norm(x)"
      ],
      "metadata": {
        "id": "C5XH48yW_yu7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"mean\",result.mean(dim=-1,keepdim=True))\n",
        "print(\"var\",result.var(dim=-1,unbiased=False,keepdim=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWqZ-ISAMJF",
        "outputId": "f5256aba-b412-4292-9865-39c9fa85a86e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean tensor([[[ 5.9605e-08],\n",
            "         [-3.9736e-08],\n",
            "         [ 0.0000e+00],\n",
            "         [ 1.9868e-08],\n",
            "         [ 3.9736e-08],\n",
            "         [-9.9341e-08],\n",
            "         [ 0.0000e+00],\n",
            "         [ 1.1921e-07],\n",
            "         [-1.5895e-07],\n",
            "         [ 0.0000e+00]],\n",
            "\n",
            "        [[-3.9736e-08],\n",
            "         [-5.9605e-08],\n",
            "         [ 2.1855e-07],\n",
            "         [ 1.6888e-07],\n",
            "         [-1.1921e-07],\n",
            "         [ 1.1921e-07],\n",
            "         [ 1.9868e-08],\n",
            "         [ 9.9341e-09],\n",
            "         [ 7.9473e-08],\n",
            "         [ 1.9868e-08]],\n",
            "\n",
            "        [[-1.1921e-07],\n",
            "         [ 0.0000e+00],\n",
            "         [ 7.9473e-08],\n",
            "         [ 7.9473e-08],\n",
            "         [ 6.3578e-07],\n",
            "         [ 1.9868e-08],\n",
            "         [ 0.0000e+00],\n",
            "         [-1.1921e-07],\n",
            "         [ 0.0000e+00],\n",
            "         [ 3.9736e-08]],\n",
            "\n",
            "        [[ 0.0000e+00],\n",
            "         [-1.1921e-07],\n",
            "         [ 0.0000e+00],\n",
            "         [-3.9736e-08],\n",
            "         [-7.4506e-08],\n",
            "         [ 0.0000e+00],\n",
            "         [ 0.0000e+00],\n",
            "         [ 0.0000e+00],\n",
            "         [ 3.9736e-08],\n",
            "         [ 1.1921e-07]],\n",
            "\n",
            "        [[ 0.0000e+00],\n",
            "         [ 2.3842e-07],\n",
            "         [ 7.9473e-08],\n",
            "         [ 5.9605e-08],\n",
            "         [ 1.1921e-07],\n",
            "         [ 1.3908e-07],\n",
            "         [ 3.5763e-07],\n",
            "         [ 3.9736e-08],\n",
            "         [ 9.9341e-08],\n",
            "         [ 5.9605e-08]]], grad_fn=<MeanBackward1>)\n",
            "var tensor([[[1.0000],\n",
            "         [0.9999],\n",
            "         [1.0000],\n",
            "         [0.9995],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000]],\n",
            "\n",
            "        [[1.0000],\n",
            "         [1.0000],\n",
            "         [0.9999],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000]],\n",
            "\n",
            "        [[1.0000],\n",
            "         [0.9999],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [0.9999],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [0.9999],\n",
            "         [1.0000]],\n",
            "\n",
            "        [[1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000]],\n",
            "\n",
            "        [[0.9999],\n",
            "         [0.9998],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [0.9999],\n",
            "         [1.0000],\n",
            "         [1.0000],\n",
            "         [1.0000]]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see based on the results, the layer normalization code works as expected and normalizes the values of each of the two inputs such that they have a mean of 0 and a variance of 1:"
      ],
      "metadata": {
        "id": "q427TkAoB09T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wR5S6OY1BZk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}