{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAtDfQV9VNh/xYg8xSLmtc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sana-Harshitha/LLMPractice/blob/main/selfAttention_with_trainable_weightsipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v9BTBzcMRA8I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_in=inputs.shape[1]\n",
        "d_out=2"
      ],
      "metadata": {
        "id": "Xq9-4OoclQOH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in GPT-like models, the input and output dimensions are usually the same.\n",
        "\n",
        "But for illustration purposes, to better follow the computation, we choose different input (d_in=3) and output (d_out=2) dimensions here.\n",
        "\n",
        "Next, we initialize the three weight matrices Wq, Wk and Wv"
      ],
      "metadata": {
        "id": "AGLjWwxLltMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "kSewa_GUlish"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.manual_seed(123)**\n",
        "\n",
        "This sets the random seed to 123, which ensures reproducibility.\n",
        "That means every time you run the code, the random numbers generated (like for weights) will be the same, so your model behaves predictably during experimentation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**torch.rand(d_in, d_out) **\n",
        "\n",
        "This creates a tensor of shape [d_in, d_out].\n",
        "It’s filled with random values between 0 and 1.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "torch.nn.Parameter(...)\n",
        "\n",
        "Wraps a Tensor as a special object used in neural networks.\n",
        "It registers the tensor as a trainable parameter in the model by default.\n",
        "So if requires_grad=True, PyTorch will update this during backpropagation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "requires_grad=False\n",
        "\n",
        "This tells PyTorch: Do not compute gradients for this parameter.\n",
        "So it won’t be updated during training.\n",
        "It becomes a fixed/static tensor, useful for:\n",
        "frozen embeddings,\n",
        "pre-defined weight matrices (for visualization or teaching),\n",
        "constant transformations.\n"
      ],
      "metadata": {
        "id": "uQw3lGgCl8RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_query)\n",
        "print(W_key)\n",
        "print(W_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN3LGQ4qlyKx",
        "outputId": "983cc47f-6967-485b-cdae-35e4292c259f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_2=inputs[1]   #[0.55, 0.87, 0.66], # journey  (x^2)\n",
        "x_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrv59dkvuGLL",
        "outputId": "54feb264-324a-4b37-f5eb-e0f0cae08084"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5500, 0.8700, 0.6600])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg9oh72ZnUg7",
        "outputId": "0e71de71-9b86-424d-d27f-bf90e589b64d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see based on the output for the query, this results in a 2-dimensional vector.\n",
        "\n",
        "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
        "\n",
        "Even though our temporary goal is to only compute the one context vector z(2), we still require the key and value vectors for all input elements.\n",
        "\n",
        "This is because they are involved in computing the attention weights with respect to the query q(2)\n",
        "\n",
        "We can obtain all keys and values via matrix multiplication:"
      ],
      "metadata": {
        "id": "2pKiMO-vua1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "queries = inputs @ W_query\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "print(\"queries.shape:\", queries.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRDNo1tst2W8",
        "outputId": "7a1fbe5b-14d2-4a0f-b738-94556141452e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n",
            "queries.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lets compute attention scores\n"
      ],
      "metadata": {
        "id": "yzmQPhCYu-To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1] #A\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8dBuOAAwe-M",
        "outputId": "bf22731a-4627-41c4-9af6-b86c56fc83da"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T #all atnention scores for given query\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CRF4z47uAGA",
        "outputId": "4061afc8-e318-46bf-f216-aa2cab893ad0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores= queries @ keys.T #all atnention scores for given query\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwpjVqBpvxbw",
        "outputId": "7fec998b-7a3a-4b1e-9bb9-e068b4da9a08"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computing attention weights by normalizing attention scores"
      ],
      "metadata": {
        "id": "Exs8uuyJxlDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the attention weights by scaling the attention scores and using the softmax function we used earlier.\n",
        "\n",
        "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys.\n",
        "\n",
        "Note that taking the square root is mathematically the same as exponentiating by 0.5:"
      ],
      "metadata": {
        "id": "lSc_oUI9yJpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]  #2\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9zi82qlxXkO",
        "outputId": "28aecd93-62e7-4723-b9de-f25c6e213f27"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rationale behind scaled-dot product attention\n",
        "The reason for the normalization by the embedding dimension size is to improve the\n",
        "training performance by avoiding small gradients. For instance, when scaling up the\n",
        "embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large\n",
        "dot products can result in very small gradients during backpropagation due to the\n",
        "softmax function applied to them. As dot products increase, the softmax function\n",
        "behaves more like a step function, resulting in gradients nearing zero. These small\n",
        "gradients can drastically slow down learning or cause training to stagnate.\n",
        "The scaling by the square root of the embedding dimension is the reason why this\n",
        "self-attention mechanism is also called scaled-dot product attention."
      ],
      "metadata": {
        "id": "jG_UvdU24fJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNmzMd9k48_s",
        "outputId": "971eba86-a007-44d9-e83e-5889bf7b9c0f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
            "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
            "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
            "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
            "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
            "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating Context Vector"
      ],
      "metadata": {
        "id": "VwqJ_QSk5cz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector = attn_weights @ values\n",
        "context_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfeTLpFuyaHV",
        "outputId": "34fcacc1-5ae4-4f51-acec-523658505844"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2996, 0.8053],\n",
              "        [0.3061, 0.8210],\n",
              "        [0.3058, 0.8203],\n",
              "        [0.2948, 0.7939],\n",
              "        [0.2927, 0.7891],\n",
              "        [0.2990, 0.8040]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Self attention on Python class"
      ],
      "metadata": {
        "id": "f4YClYRtAxLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "CeCkHAPcBCDl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management.\n",
        "\n",
        "The init method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
        "\n",
        "During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
        "\n",
        "Finally, we create a context vector by weighting the values with these normalized attention scores."
      ],
      "metadata": {
        "id": "VrTj-5yyBt_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))   #pytorch internally calls  \"\"sa_v1.__call__(inputs)\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMlHeua2BDSi",
        "outputId": "9ee88b87-d214-4f64-8740-2f7137a96249"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled.\n",
        "\n",
        "Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training."
      ],
      "metadata": {
        "id": "7kmNSm7Tdy9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "dx8uVsJjdK-S"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkK7GzGPBxFq",
        "outputId": "dfc9725b-b82f-4545-bb91-79aa3be40c5f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since nn.Linear uses a more sophisticated weight initialization scheme."
      ],
      "metadata": {
        "id": "DlzjcWyeeBCW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PwHSXIRdUcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}